{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import gzip\n",
    "import itertools\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import lasagne\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_FILENAME = 'mnist.pkl.gz'\n",
    "NUM_EPOCHS = 200\n",
    "BATCH_SIZE = 600\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(data):\n",
    "    X_train, y_train = data[0]\n",
    "    X_valid, y_valid = data[1]\n",
    "    X_test, y_test = data[2]\n",
    "\n",
    "    # reshape for convolutions\n",
    "    X_train = X_train.reshape((X_train.shape[0], 1, 28, 28))\n",
    "    X_valid = X_valid.reshape((X_valid.shape[0], 1, 28, 28))\n",
    "    X_test = X_test.reshape((X_test.shape[0], 1, 28, 28))\n",
    "\n",
    "    return dict(\n",
    "        X_train=theano.shared(lasagne.utils.floatX(X_train)),\n",
    "        y_train=T.cast(theano.shared(y_train), 'int32'),\n",
    "        X_valid=theano.shared(lasagne.utils.floatX(X_valid)),\n",
    "        y_valid=T.cast(theano.shared(y_valid), 'int32'),\n",
    "        X_test=theano.shared(lasagne.utils.floatX(X_test)),\n",
    "        y_test=T.cast(theano.shared(y_test), 'int32'),\n",
    "        num_examples_train=X_train.shape[0],\n",
    "        num_examples_valid=X_valid.shape[0],\n",
    "        num_examples_test=X_test.shape[0],\n",
    "        input_height=X_train.shape[2],\n",
    "        input_width=X_train.shape[3],\n",
    "        output_dim=10,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(input_width, input_height, output_dim,\n",
    "                batch_size=BATCH_SIZE):\n",
    "    l_in = lasagne.layers.InputLayer(\n",
    "        shape=(batch_size, 1, input_width, input_height),\n",
    "        )\n",
    "\n",
    "    l_conv1 = lasagne.layers.Conv2DLayer(\n",
    "        l_in,\n",
    "        num_filters=32,\n",
    "        filter_size=(5, 5),\n",
    "        nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        W=lasagne.init.GlorotUniform(),\n",
    "        )\n",
    "    l_pool1 = lasagne.layers.MaxPool2DLayer(l_conv1, pool_size=(2, 2))\n",
    "\n",
    "    l_conv2 = lasagne.layers.Conv2DLayer(\n",
    "        l_pool1,\n",
    "        num_filters=32,\n",
    "        filter_size=(5, 5),\n",
    "        nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        W=lasagne.init.GlorotUniform(),\n",
    "        )\n",
    "    l_pool2 = lasagne.layers.MaxPool2DLayer(l_conv2, pool_size=(2, 2))\n",
    "\n",
    "    l_hidden1 = lasagne.layers.DenseLayer(\n",
    "        l_pool2,\n",
    "        num_units=256,\n",
    "        nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        W=lasagne.init.GlorotUniform(),\n",
    "        )\n",
    "\n",
    "    l_hidden1_dropout = lasagne.layers.DropoutLayer(l_hidden1, p=0.5)\n",
    "\n",
    "    # l_hidden2 = lasagne.layers.DenseLayer(\n",
    "    #     l_hidden1_dropout,\n",
    "    #     num_units=256,\n",
    "    #     nonlinearity=lasagne.nonlinearities.rectify,\n",
    "    #     )\n",
    "    # l_hidden2_dropout = lasagne.layers.DropoutLayer(l_hidden2, p=0.5)\n",
    "\n",
    "    l_out = lasagne.layers.DenseLayer(\n",
    "        l_hidden1_dropout,\n",
    "        num_units=output_dim,\n",
    "        nonlinearity=lasagne.nonlinearities.softmax,\n",
    "        W=lasagne.init.GlorotUniform(),\n",
    "        )\n",
    "\n",
    "    return l_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_iter_functions(dataset, output_layer,\n",
    "                          X_tensor_type=T.matrix,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          learning_rate=LEARNING_RATE, momentum=MOMENTUM):\n",
    "    \"\"\"Create functions for training, validation and testing to iterate one\n",
    "       epoch.\n",
    "    \"\"\"\n",
    "    batch_index = T.iscalar('batch_index')\n",
    "    X_batch = X_tensor_type('x')\n",
    "    y_batch = T.ivector('y')\n",
    "    batch_slice = slice(batch_index * batch_size,\n",
    "                        (batch_index + 1) * batch_size)\n",
    "\n",
    "    objective = lasagne.objectives.Objective(output_layer,\n",
    "        loss_function=lasagne.objectives.categorical_crossentropy)\n",
    "\n",
    "    loss_train = objective.get_loss(X_batch, target=y_batch)\n",
    "    loss_eval = objective.get_loss(X_batch, target=y_batch,\n",
    "                                   deterministic=True)\n",
    "\n",
    "    pred = T.argmax(\n",
    "        lasagne.layers.get_output(output_layer, X_batch, deterministic=True),\n",
    "        axis=1)\n",
    "    accuracy = T.mean(T.eq(pred, y_batch), dtype=theano.config.floatX)\n",
    "\n",
    "    all_params = lasagne.layers.get_all_params(output_layer)\n",
    "    updates = lasagne.updates.nesterov_momentum(\n",
    "        loss_train, all_params, learning_rate, momentum)\n",
    "\n",
    "    iter_train = theano.function(\n",
    "        [batch_index], loss_train,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            X_batch: dataset['X_train'][batch_slice],\n",
    "            y_batch: dataset['y_train'][batch_slice],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    iter_valid = theano.function(\n",
    "        [batch_index], [loss_eval, accuracy],\n",
    "        givens={\n",
    "            X_batch: dataset['X_valid'][batch_slice],\n",
    "            y_batch: dataset['y_valid'][batch_slice],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    iter_test = theano.function(\n",
    "        [batch_index], [loss_eval, accuracy],\n",
    "        givens={\n",
    "            X_batch: dataset['X_test'][batch_slice],\n",
    "            y_batch: dataset['y_test'][batch_slice],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return dict(\n",
    "        train=iter_train,\n",
    "        valid=iter_valid,\n",
    "        test=iter_test,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(iter_funcs, dataset, batch_size=BATCH_SIZE):\n",
    "    \"\"\"Train the model with `dataset` with mini-batch training. Each\n",
    "       mini-batch has `batch_size` recordings.\n",
    "    \"\"\"\n",
    "    num_batches_train = dataset['num_examples_train'] // batch_size\n",
    "    num_batches_valid = dataset['num_examples_valid'] // batch_size\n",
    "\n",
    "    for epoch in itertools.count(1):\n",
    "        batch_train_losses = []\n",
    "        for b in range(num_batches_train):\n",
    "            batch_train_loss = iter_funcs['train'](b)\n",
    "            batch_train_losses.append(batch_train_loss)\n",
    "\n",
    "        avg_train_loss = np.mean(batch_train_losses)\n",
    "\n",
    "        batch_valid_losses = []\n",
    "        batch_valid_accuracies = []\n",
    "        for b in range(num_batches_valid):\n",
    "            batch_valid_loss, batch_valid_accuracy = iter_funcs['valid'](b)\n",
    "            batch_valid_losses.append(batch_valid_loss)\n",
    "            batch_valid_accuracies.append(batch_valid_accuracy)\n",
    "\n",
    "        avg_valid_loss = np.mean(batch_valid_losses)\n",
    "        avg_valid_accuracy = np.mean(batch_valid_accuracies)\n",
    "\n",
    "        yield {\n",
    "            'number': epoch,\n",
    "            'train_loss': avg_train_loss,\n",
    "            'valid_loss': avg_valid_loss,\n",
    "            'valid_accuracy': avg_valid_accuracy,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Building model and compiling functions...\n",
      "Starting training...\n",
      "Epoch 1 of 200 took 174.946s\n",
      "  training loss:\t\t1.242629\n",
      "  validation loss:\t\t0.271462\n",
      "  validation accuracy:\t\t91.96 %%\n",
      "Epoch 2 of 200 took 200.097s\n",
      "  training loss:\t\t0.312536\n",
      "  validation loss:\t\t0.155670\n",
      "  validation accuracy:\t\t95.55 %%\n",
      "Epoch 3 of 200 took 184.285s\n",
      "  training loss:\t\t0.213655\n",
      "  validation loss:\t\t0.118474\n",
      "  validation accuracy:\t\t96.52 %%\n",
      "Epoch 4 of 200 took 177.598s\n",
      "  training loss:\t\t0.169883\n",
      "  validation loss:\t\t0.096493\n",
      "  validation accuracy:\t\t97.16 %%\n",
      "Epoch 5 of 200 took 178.313s\n",
      "  training loss:\t\t0.146444\n",
      "  validation loss:\t\t0.083608\n",
      "  validation accuracy:\t\t97.43 %%\n",
      "Epoch 6 of 200 took 177.332s\n",
      "  training loss:\t\t0.128278\n",
      "  validation loss:\t\t0.075484\n",
      "  validation accuracy:\t\t97.62 %%\n",
      "Epoch 7 of 200 took 176.976s\n",
      "  training loss:\t\t0.115729\n",
      "  validation loss:\t\t0.068976\n",
      "  validation accuracy:\t\t97.85 %%\n",
      "Epoch 8 of 200 took 174.416s\n",
      "  training loss:\t\t0.105179\n",
      "  validation loss:\t\t0.063809\n",
      "  validation accuracy:\t\t97.98 %%\n",
      "Epoch 9 of 200 took 174.995s\n",
      "  training loss:\t\t0.096124\n",
      "  validation loss:\t\t0.059951\n",
      "  validation accuracy:\t\t98.05 %%\n",
      "Epoch 10 of 200 took 174.222s\n",
      "  training loss:\t\t0.091027\n",
      "  validation loss:\t\t0.057243\n",
      "  validation accuracy:\t\t98.16 %%\n",
      "Epoch 11 of 200 took 174.762s\n",
      "  training loss:\t\t0.084521\n",
      "  validation loss:\t\t0.054204\n",
      "  validation accuracy:\t\t98.23 %%\n",
      "Epoch 12 of 200 took 180.207s\n",
      "  training loss:\t\t0.080658\n",
      "  validation loss:\t\t0.051951\n",
      "  validation accuracy:\t\t98.29 %%\n",
      "Epoch 13 of 200 took 174.119s\n",
      "  training loss:\t\t0.074935\n",
      "  validation loss:\t\t0.049074\n",
      "  validation accuracy:\t\t98.39 %%\n",
      "Epoch 14 of 200 took 173.336s\n",
      "  training loss:\t\t0.073975\n",
      "  validation loss:\t\t0.047464\n",
      "  validation accuracy:\t\t98.49 %%\n",
      "Epoch 15 of 200 took 175.313s\n",
      "  training loss:\t\t0.071554\n",
      "  validation loss:\t\t0.046353\n",
      "  validation accuracy:\t\t98.52 %%\n",
      "Epoch 16 of 200 took 173.701s\n",
      "  training loss:\t\t0.067563\n",
      "  validation loss:\t\t0.045016\n",
      "  validation accuracy:\t\t98.55 %%\n",
      "Epoch 17 of 200 took 173.395s\n",
      "  training loss:\t\t0.064036\n",
      "  validation loss:\t\t0.044432\n",
      "  validation accuracy:\t\t98.55 %%\n",
      "Epoch 18 of 200 took 177.765s\n",
      "  training loss:\t\t0.062768\n",
      "  validation loss:\t\t0.042111\n",
      "  validation accuracy:\t\t98.67 %%\n",
      "Epoch 19 of 200 took 187.817s\n",
      "  training loss:\t\t0.059874\n",
      "  validation loss:\t\t0.041446\n",
      "  validation accuracy:\t\t98.64 %%\n",
      "Epoch 20 of 200 took 181.003s\n",
      "  training loss:\t\t0.055150\n",
      "  validation loss:\t\t0.041254\n",
      "  validation accuracy:\t\t98.58 %%\n",
      "Epoch 21 of 200 took 176.452s\n",
      "  training loss:\t\t0.054561\n",
      "  validation loss:\t\t0.040863\n",
      "  validation accuracy:\t\t98.69 %%\n",
      "Epoch 22 of 200 took 175.830s\n",
      "  training loss:\t\t0.053435\n",
      "  validation loss:\t\t0.039351\n",
      "  validation accuracy:\t\t98.74 %%\n",
      "Epoch 23 of 200 took 176.127s\n",
      "  training loss:\t\t0.051261\n",
      "  validation loss:\t\t0.038018\n",
      "  validation accuracy:\t\t98.76 %%\n",
      "Epoch 24 of 200 took 173.406s\n",
      "  training loss:\t\t0.051460\n",
      "  validation loss:\t\t0.038752\n",
      "  validation accuracy:\t\t98.79 %%\n",
      "Epoch 25 of 200 took 175.422s\n",
      "  training loss:\t\t0.048922\n",
      "  validation loss:\t\t0.037568\n",
      "  validation accuracy:\t\t98.71 %%\n",
      "Epoch 26 of 200 took 174.217s\n",
      "  training loss:\t\t0.047735\n",
      "  validation loss:\t\t0.036557\n",
      "  validation accuracy:\t\t98.84 %%\n",
      "Epoch 27 of 200 took 173.807s\n",
      "  training loss:\t\t0.046517\n",
      "  validation loss:\t\t0.038296\n",
      "  validation accuracy:\t\t98.78 %%\n",
      "Epoch 28 of 200 took 173.757s\n",
      "  training loss:\t\t0.045695\n",
      "  validation loss:\t\t0.038220\n",
      "  validation accuracy:\t\t98.75 %%\n",
      "Epoch 29 of 200 took 177.346s\n",
      "  training loss:\t\t0.043490\n",
      "  validation loss:\t\t0.036492\n",
      "  validation accuracy:\t\t98.82 %%\n",
      "Epoch 30 of 200 took 190.207s\n",
      "  training loss:\t\t0.042334\n",
      "  validation loss:\t\t0.035961\n",
      "  validation accuracy:\t\t98.83 %%\n",
      "Epoch 31 of 200 took 194.386s\n",
      "  training loss:\t\t0.042641\n",
      "  validation loss:\t\t0.035814\n",
      "  validation accuracy:\t\t98.83 %%\n",
      "Epoch 32 of 200 took 183.520s\n",
      "  training loss:\t\t0.040549\n",
      "  validation loss:\t\t0.035080\n",
      "  validation accuracy:\t\t98.83 %%\n",
      "Epoch 33 of 200 took 200.670s\n",
      "  training loss:\t\t0.039924\n",
      "  validation loss:\t\t0.035014\n",
      "  validation accuracy:\t\t98.81 %%\n",
      "Epoch 34 of 200 took 213.694s\n",
      "  training loss:\t\t0.037751\n",
      "  validation loss:\t\t0.035072\n",
      "  validation accuracy:\t\t98.89 %%\n",
      "Epoch 35 of 200 took 209.604s\n",
      "  training loss:\t\t0.037396\n",
      "  validation loss:\t\t0.034472\n",
      "  validation accuracy:\t\t98.89 %%\n",
      "Epoch 36 of 200 took 207.461s\n",
      "  training loss:\t\t0.037071\n",
      "  validation loss:\t\t0.034251\n",
      "  validation accuracy:\t\t98.91 %%\n",
      "Epoch 37 of 200 took 223.871s\n",
      "  training loss:\t\t0.036224\n",
      "  validation loss:\t\t0.034188\n",
      "  validation accuracy:\t\t98.85 %%\n",
      "Epoch 38 of 200 took 213.409s\n",
      "  training loss:\t\t0.036672\n",
      "  validation loss:\t\t0.034382\n",
      "  validation accuracy:\t\t98.88 %%\n",
      "Epoch 39 of 200 took 210.354s\n",
      "  training loss:\t\t0.036229\n",
      "  validation loss:\t\t0.032623\n",
      "  validation accuracy:\t\t98.90 %%\n",
      "Epoch 40 of 200 took 227.577s\n",
      "  training loss:\t\t0.032412\n",
      "  validation loss:\t\t0.032998\n",
      "  validation accuracy:\t\t98.93 %%\n",
      "Epoch 41 of 200 took 233.708s\n",
      "  training loss:\t\t0.033724\n",
      "  validation loss:\t\t0.032734\n",
      "  validation accuracy:\t\t98.91 %%\n",
      "Epoch 42 of 200 took 232.047s\n",
      "  training loss:\t\t0.032555\n",
      "  validation loss:\t\t0.031924\n",
      "  validation accuracy:\t\t98.94 %%\n",
      "Epoch 43 of 200 took 224.897s\n",
      "  training loss:\t\t0.031350\n",
      "  validation loss:\t\t0.032070\n",
      "  validation accuracy:\t\t98.91 %%\n",
      "Epoch 44 of 200 took 211.136s\n",
      "  training loss:\t\t0.030594\n",
      "  validation loss:\t\t0.031882\n",
      "  validation accuracy:\t\t98.98 %%\n",
      "Epoch 45 of 200 took 215.065s\n",
      "  training loss:\t\t0.031042\n",
      "  validation loss:\t\t0.031792\n",
      "  validation accuracy:\t\t98.95 %%\n",
      "Epoch 46 of 200 took 193.095s\n",
      "  training loss:\t\t0.030222\n",
      "  validation loss:\t\t0.032438\n",
      "  validation accuracy:\t\t98.95 %%\n",
      "Epoch 47 of 200 took 211.300s\n",
      "  training loss:\t\t0.029151\n",
      "  validation loss:\t\t0.031527\n",
      "  validation accuracy:\t\t98.99 %%\n",
      "Epoch 48 of 200 took 211.457s\n",
      "  training loss:\t\t0.028870\n",
      "  validation loss:\t\t0.030687\n",
      "  validation accuracy:\t\t99.00 %%\n",
      "Epoch 49 of 200 took 212.478s\n",
      "  training loss:\t\t0.029732\n",
      "  validation loss:\t\t0.032273\n",
      "  validation accuracy:\t\t99.03 %%\n",
      "Epoch 50 of 200 took 217.122s\n",
      "  training loss:\t\t0.028872\n",
      "  validation loss:\t\t0.031430\n",
      "  validation accuracy:\t\t99.00 %%\n",
      "Epoch 51 of 200 took 211.695s\n",
      "  training loss:\t\t0.028159\n",
      "  validation loss:\t\t0.031146\n",
      "  validation accuracy:\t\t98.98 %%\n",
      "Epoch 52 of 200 took 214.241s\n",
      "  training loss:\t\t0.026880\n",
      "  validation loss:\t\t0.030874\n",
      "  validation accuracy:\t\t98.98 %%\n",
      "Epoch 53 of 200 took 199.208s\n",
      "  training loss:\t\t0.027076\n",
      "  validation loss:\t\t0.030525\n",
      "  validation accuracy:\t\t99.01 %%\n",
      "Epoch 54 of 200 took 177.037s\n",
      "  training loss:\t\t0.025250\n",
      "  validation loss:\t\t0.031327\n",
      "  validation accuracy:\t\t98.98 %%\n",
      "Epoch 55 of 200 took 173.345s\n",
      "  training loss:\t\t0.025180\n",
      "  validation loss:\t\t0.030590\n",
      "  validation accuracy:\t\t99.05 %%\n",
      "Epoch 56 of 200 took 174.460s\n",
      "  training loss:\t\t0.024713\n",
      "  validation loss:\t\t0.030517\n",
      "  validation accuracy:\t\t99.15 %%\n",
      "Epoch 57 of 200 took 173.859s\n",
      "  training loss:\t\t0.024192\n",
      "  validation loss:\t\t0.032367\n",
      "  validation accuracy:\t\t98.96 %%\n",
      "Epoch 58 of 200 took 173.373s\n",
      "  training loss:\t\t0.022888\n",
      "  validation loss:\t\t0.030038\n",
      "  validation accuracy:\t\t99.05 %%\n",
      "Epoch 59 of 200 took 173.642s\n",
      "  training loss:\t\t0.024369\n",
      "  validation loss:\t\t0.029788\n",
      "  validation accuracy:\t\t99.02 %%\n",
      "Epoch 60 of 200 took 173.307s\n",
      "  training loss:\t\t0.024130\n",
      "  validation loss:\t\t0.031348\n",
      "  validation accuracy:\t\t99.03 %%\n",
      "Epoch 61 of 200 took 177.779s\n",
      "  training loss:\t\t0.023749\n",
      "  validation loss:\t\t0.031098\n",
      "  validation accuracy:\t\t99.03 %%\n",
      "Epoch 62 of 200 took 173.809s\n",
      "  training loss:\t\t0.023018\n",
      "  validation loss:\t\t0.029426\n",
      "  validation accuracy:\t\t99.15 %%\n",
      "Epoch 63 of 200 took 173.768s\n",
      "  training loss:\t\t0.022541\n",
      "  validation loss:\t\t0.030060\n",
      "  validation accuracy:\t\t99.04 %%\n",
      "Epoch 64 of 200 took 173.607s\n",
      "  training loss:\t\t0.023230\n",
      "  validation loss:\t\t0.029961\n",
      "  validation accuracy:\t\t99.02 %%\n",
      "Epoch 65 of 200 took 173.441s\n",
      "  training loss:\t\t0.022102\n",
      "  validation loss:\t\t0.030130\n",
      "  validation accuracy:\t\t99.02 %%\n",
      "Epoch 66 of 200 took 173.400s\n",
      "  training loss:\t\t0.021488\n",
      "  validation loss:\t\t0.030411\n",
      "  validation accuracy:\t\t99.08 %%\n",
      "Epoch 67 of 200 took 173.466s\n",
      "  training loss:\t\t0.020972\n",
      "  validation loss:\t\t0.031858\n",
      "  validation accuracy:\t\t99.07 %%\n",
      "Epoch 68 of 200 took 173.515s\n",
      "  training loss:\t\t0.020636\n",
      "  validation loss:\t\t0.030347\n",
      "  validation accuracy:\t\t99.07 %%\n",
      "Epoch 69 of 200 took 173.349s\n",
      "  training loss:\t\t0.019832\n",
      "  validation loss:\t\t0.030090\n",
      "  validation accuracy:\t\t99.02 %%\n",
      "Epoch 70 of 200 took 173.625s\n",
      "  training loss:\t\t0.019862\n",
      "  validation loss:\t\t0.031033\n",
      "  validation accuracy:\t\t99.05 %%\n",
      "Epoch 71 of 200 took 172.969s\n",
      "  training loss:\t\t0.020034\n",
      "  validation loss:\t\t0.031447\n",
      "  validation accuracy:\t\t98.98 %%\n",
      "Epoch 72 of 200 took 173.418s\n",
      "  training loss:\t\t0.019389\n",
      "  validation loss:\t\t0.030132\n",
      "  validation accuracy:\t\t99.05 %%\n",
      "Epoch 73 of 200 took 173.417s\n",
      "  training loss:\t\t0.019370\n",
      "  validation loss:\t\t0.030179\n",
      "  validation accuracy:\t\t99.07 %%\n",
      "Epoch 74 of 200 took 173.494s\n",
      "  training loss:\t\t0.018577\n",
      "  validation loss:\t\t0.029999\n",
      "  validation accuracy:\t\t99.02 %%\n",
      "Epoch 75 of 200 took 173.757s\n",
      "  training loss:\t\t0.018030\n",
      "  validation loss:\t\t0.030717\n",
      "  validation accuracy:\t\t99.10 %%\n",
      "Epoch 76 of 200 took 173.805s\n",
      "  training loss:\t\t0.018003\n",
      "  validation loss:\t\t0.032598\n",
      "  validation accuracy:\t\t99.07 %%\n",
      "Epoch 77 of 200 took 174.282s\n",
      "  training loss:\t\t0.018229\n",
      "  validation loss:\t\t0.031366\n",
      "  validation accuracy:\t\t99.07 %%\n",
      "Epoch 78 of 200 took 173.229s\n",
      "  training loss:\t\t0.018791\n",
      "  validation loss:\t\t0.030535\n",
      "  validation accuracy:\t\t99.04 %%\n",
      "Epoch 79 of 200 took 173.326s\n",
      "  training loss:\t\t0.017671\n",
      "  validation loss:\t\t0.029910\n",
      "  validation accuracy:\t\t99.05 %%\n",
      "Epoch 80 of 200 took 173.271s\n",
      "  training loss:\t\t0.016922\n",
      "  validation loss:\t\t0.029760\n",
      "  validation accuracy:\t\t99.05 %%\n",
      "Epoch 81 of 200 took 188.473s\n",
      "  training loss:\t\t0.016296\n",
      "  validation loss:\t\t0.030195\n",
      "  validation accuracy:\t\t99.04 %%\n",
      "Epoch 82 of 200 took 177.257s\n",
      "  training loss:\t\t0.016402\n",
      "  validation loss:\t\t0.032094\n",
      "  validation accuracy:\t\t99.03 %%\n",
      "Epoch 83 of 200 took 173.379s\n",
      "  training loss:\t\t0.015976\n",
      "  validation loss:\t\t0.031474\n",
      "  validation accuracy:\t\t99.08 %%\n",
      "Epoch 84 of 200 took 173.666s\n",
      "  training loss:\t\t0.016204\n",
      "  validation loss:\t\t0.031227\n",
      "  validation accuracy:\t\t99.07 %%\n",
      "Epoch 85 of 200 took 173.240s\n",
      "  training loss:\t\t0.015237\n",
      "  validation loss:\t\t0.030762\n",
      "  validation accuracy:\t\t99.11 %%\n",
      "Epoch 86 of 200 took 173.635s\n",
      "  training loss:\t\t0.016069\n",
      "  validation loss:\t\t0.031290\n",
      "  validation accuracy:\t\t99.11 %%\n",
      "Epoch 87 of 200 took 173.369s\n",
      "  training loss:\t\t0.016258\n",
      "  validation loss:\t\t0.031846\n",
      "  validation accuracy:\t\t98.97 %%\n",
      "Epoch 88 of 200 took 173.688s\n",
      "  training loss:\t\t0.016570\n",
      "  validation loss:\t\t0.029569\n",
      "  validation accuracy:\t\t99.07 %%\n",
      "Epoch 89 of 200 took 173.453s\n",
      "  training loss:\t\t0.014587\n",
      "  validation loss:\t\t0.030260\n",
      "  validation accuracy:\t\t99.10 %%\n",
      "Epoch 90 of 200 took 173.283s\n",
      "  training loss:\t\t0.015476\n",
      "  validation loss:\t\t0.030566\n",
      "  validation accuracy:\t\t99.07 %%\n",
      "Epoch 91 of 200 took 173.277s\n",
      "  training loss:\t\t0.014944\n",
      "  validation loss:\t\t0.030642\n",
      "  validation accuracy:\t\t99.07 %%\n",
      "Epoch 92 of 200 took 173.101s\n",
      "  training loss:\t\t0.015682\n",
      "  validation loss:\t\t0.030824\n",
      "  validation accuracy:\t\t99.10 %%\n",
      "Epoch 93 of 200 took 173.125s\n",
      "  training loss:\t\t0.015075\n",
      "  validation loss:\t\t0.030568\n",
      "  validation accuracy:\t\t99.07 %%\n",
      "Epoch 94 of 200 took 173.226s\n",
      "  training loss:\t\t0.014875\n",
      "  validation loss:\t\t0.030515\n",
      "  validation accuracy:\t\t99.10 %%\n",
      "Epoch 95 of 200 took 173.520s\n",
      "  training loss:\t\t0.014603\n",
      "  validation loss:\t\t0.029510\n",
      "  validation accuracy:\t\t99.09 %%\n",
      "Epoch 96 of 200 took 173.266s\n",
      "  training loss:\t\t0.013721\n",
      "  validation loss:\t\t0.029602\n",
      "  validation accuracy:\t\t99.17 %%\n",
      "Epoch 97 of 200 took 173.625s\n",
      "  training loss:\t\t0.013248\n",
      "  validation loss:\t\t0.030373\n",
      "  validation accuracy:\t\t99.17 %%\n",
      "Epoch 98 of 200 took 173.514s\n",
      "  training loss:\t\t0.013398\n",
      "  validation loss:\t\t0.029816\n",
      "  validation accuracy:\t\t99.16 %%\n",
      "Epoch 99 of 200 took 173.203s\n",
      "  training loss:\t\t0.014257\n",
      "  validation loss:\t\t0.030079\n",
      "  validation accuracy:\t\t99.14 %%\n",
      "Epoch 100 of 200 took 173.496s\n",
      "  training loss:\t\t0.012927\n",
      "  validation loss:\t\t0.029108\n",
      "  validation accuracy:\t\t99.13 %%\n",
      "Epoch 101 of 200 took 173.308s\n",
      "  training loss:\t\t0.013704\n",
      "  validation loss:\t\t0.029488\n",
      "  validation accuracy:\t\t99.17 %%\n",
      "Epoch 102 of 200 took 173.142s\n",
      "  training loss:\t\t0.013214\n",
      "  validation loss:\t\t0.031418\n",
      "  validation accuracy:\t\t99.14 %%\n",
      "Epoch 103 of 200 took 173.765s\n",
      "  training loss:\t\t0.013729\n",
      "  validation loss:\t\t0.030360\n",
      "  validation accuracy:\t\t99.09 %%\n",
      "Epoch 104 of 200 took 173.540s\n",
      "  training loss:\t\t0.012829\n",
      "  validation loss:\t\t0.030199\n",
      "  validation accuracy:\t\t99.08 %%\n",
      "Epoch 105 of 200 took 173.516s\n",
      "  training loss:\t\t0.013062\n",
      "  validation loss:\t\t0.030782\n",
      "  validation accuracy:\t\t99.19 %%\n",
      "Epoch 106 of 200 took 173.323s\n",
      "  training loss:\t\t0.011011\n",
      "  validation loss:\t\t0.030828\n",
      "  validation accuracy:\t\t99.15 %%\n",
      "Epoch 107 of 200 took 173.742s\n",
      "  training loss:\t\t0.012798\n",
      "  validation loss:\t\t0.030354\n",
      "  validation accuracy:\t\t99.11 %%\n",
      "Epoch 108 of 200 took 173.056s\n",
      "  training loss:\t\t0.012867\n",
      "  validation loss:\t\t0.030212\n",
      "  validation accuracy:\t\t99.14 %%\n",
      "Epoch 109 of 200 took 173.140s\n",
      "  training loss:\t\t0.013131\n",
      "  validation loss:\t\t0.031482\n",
      "  validation accuracy:\t\t99.15 %%\n",
      "Epoch 110 of 200 took 173.496s\n",
      "  training loss:\t\t0.012420\n",
      "  validation loss:\t\t0.032017\n",
      "  validation accuracy:\t\t99.14 %%\n",
      "Epoch 111 of 200 took 172.997s\n",
      "  training loss:\t\t0.011440\n",
      "  validation loss:\t\t0.032787\n",
      "  validation accuracy:\t\t99.09 %%\n",
      "Epoch 112 of 200 took 172.993s\n",
      "  training loss:\t\t0.011258\n",
      "  validation loss:\t\t0.031192\n",
      "  validation accuracy:\t\t99.12 %%\n",
      "Epoch 113 of 200 took 173.472s\n",
      "  training loss:\t\t0.012212\n",
      "  validation loss:\t\t0.032544\n",
      "  validation accuracy:\t\t99.13 %%\n",
      "Epoch 114 of 200 took 173.709s\n",
      "  training loss:\t\t0.011390\n",
      "  validation loss:\t\t0.031495\n",
      "  validation accuracy:\t\t99.15 %%\n",
      "Epoch 115 of 200 took 173.143s\n",
      "  training loss:\t\t0.012341\n",
      "  validation loss:\t\t0.031368\n",
      "  validation accuracy:\t\t99.11 %%\n",
      "Epoch 116 of 200 took 173.905s\n",
      "  training loss:\t\t0.011554\n",
      "  validation loss:\t\t0.031450\n",
      "  validation accuracy:\t\t99.22 %%\n",
      "Epoch 117 of 200 took 173.465s\n",
      "  training loss:\t\t0.011585\n",
      "  validation loss:\t\t0.030420\n",
      "  validation accuracy:\t\t99.18 %%\n",
      "Epoch 118 of 200 took 173.470s\n",
      "  training loss:\t\t0.011125\n",
      "  validation loss:\t\t0.030189\n",
      "  validation accuracy:\t\t99.06 %%\n",
      "Epoch 119 of 200 took 173.364s\n",
      "  training loss:\t\t0.011183\n",
      "  validation loss:\t\t0.029895\n",
      "  validation accuracy:\t\t99.14 %%\n",
      "Epoch 120 of 200 took 173.013s\n",
      "  training loss:\t\t0.010501\n",
      "  validation loss:\t\t0.031817\n",
      "  validation accuracy:\t\t99.09 %%\n",
      "Epoch 121 of 200 took 173.426s\n",
      "  training loss:\t\t0.010348\n",
      "  validation loss:\t\t0.030283\n",
      "  validation accuracy:\t\t99.13 %%\n",
      "Epoch 122 of 200 took 173.486s\n",
      "  training loss:\t\t0.011029\n",
      "  validation loss:\t\t0.030284\n",
      "  validation accuracy:\t\t99.21 %%\n",
      "Epoch 123 of 200 took 173.068s\n",
      "  training loss:\t\t0.010203\n",
      "  validation loss:\t\t0.030166\n",
      "  validation accuracy:\t\t99.13 %%\n",
      "Epoch 124 of 200 took 173.361s\n",
      "  training loss:\t\t0.010677\n",
      "  validation loss:\t\t0.029371\n",
      "  validation accuracy:\t\t99.21 %%\n",
      "Epoch 125 of 200 took 173.556s\n",
      "  training loss:\t\t0.010247\n",
      "  validation loss:\t\t0.031291\n",
      "  validation accuracy:\t\t99.14 %%\n",
      "Epoch 126 of 200 took 173.440s\n",
      "  training loss:\t\t0.010270\n",
      "  validation loss:\t\t0.030648\n",
      "  validation accuracy:\t\t99.18 %%\n",
      "Epoch 127 of 200 took 173.519s\n",
      "  training loss:\t\t0.009961\n",
      "  validation loss:\t\t0.029987\n",
      "  validation accuracy:\t\t99.20 %%\n",
      "Epoch 128 of 200 took 173.411s\n",
      "  training loss:\t\t0.010196\n",
      "  validation loss:\t\t0.030124\n",
      "  validation accuracy:\t\t99.19 %%\n",
      "Epoch 129 of 200 took 173.282s\n",
      "  training loss:\t\t0.010351\n",
      "  validation loss:\t\t0.031019\n",
      "  validation accuracy:\t\t99.18 %%\n",
      "Epoch 130 of 200 took 173.441s\n",
      "  training loss:\t\t0.009544\n",
      "  validation loss:\t\t0.029925\n",
      "  validation accuracy:\t\t99.16 %%\n",
      "Epoch 131 of 200 took 173.688s\n",
      "  training loss:\t\t0.009518\n",
      "  validation loss:\t\t0.030982\n",
      "  validation accuracy:\t\t99.14 %%\n",
      "Epoch 132 of 200 took 173.318s\n",
      "  training loss:\t\t0.009921\n",
      "  validation loss:\t\t0.031488\n",
      "  validation accuracy:\t\t99.07 %%\n",
      "Epoch 133 of 200 took 173.605s\n",
      "  training loss:\t\t0.009429\n",
      "  validation loss:\t\t0.033190\n",
      "  validation accuracy:\t\t99.07 %%\n",
      "Epoch 134 of 200 took 173.292s\n",
      "  training loss:\t\t0.009619\n",
      "  validation loss:\t\t0.031526\n",
      "  validation accuracy:\t\t99.09 %%\n",
      "Epoch 135 of 200 took 173.586s\n",
      "  training loss:\t\t0.009090\n",
      "  validation loss:\t\t0.030581\n",
      "  validation accuracy:\t\t99.13 %%\n",
      "Epoch 136 of 200 took 173.902s\n",
      "  training loss:\t\t0.008871\n",
      "  validation loss:\t\t0.032506\n",
      "  validation accuracy:\t\t99.11 %%\n",
      "Epoch 137 of 200 took 173.446s\n",
      "  training loss:\t\t0.008844\n",
      "  validation loss:\t\t0.032717\n",
      "  validation accuracy:\t\t99.16 %%\n",
      "Epoch 138 of 200 took 173.528s\n",
      "  training loss:\t\t0.008788\n",
      "  validation loss:\t\t0.031533\n",
      "  validation accuracy:\t\t99.16 %%\n",
      "Epoch 139 of 200 took 173.068s\n",
      "  training loss:\t\t0.008951\n",
      "  validation loss:\t\t0.031313\n",
      "  validation accuracy:\t\t99.16 %%\n",
      "Epoch 140 of 200 took 173.499s\n",
      "  training loss:\t\t0.008819\n",
      "  validation loss:\t\t0.032017\n",
      "  validation accuracy:\t\t99.06 %%\n",
      "Epoch 141 of 200 took 173.498s\n",
      "  training loss:\t\t0.009247\n",
      "  validation loss:\t\t0.030869\n",
      "  validation accuracy:\t\t99.18 %%\n",
      "Epoch 142 of 200 took 173.311s\n",
      "  training loss:\t\t0.008288\n",
      "  validation loss:\t\t0.032200\n",
      "  validation accuracy:\t\t99.09 %%\n",
      "Epoch 143 of 200 took 172.996s\n",
      "  training loss:\t\t0.009039\n",
      "  validation loss:\t\t0.030575\n",
      "  validation accuracy:\t\t99.16 %%\n",
      "Epoch 144 of 200 took 173.492s\n",
      "  training loss:\t\t0.009006\n",
      "  validation loss:\t\t0.032816\n",
      "  validation accuracy:\t\t99.17 %%\n",
      "Epoch 145 of 200 took 173.828s\n",
      "  training loss:\t\t0.008361\n",
      "  validation loss:\t\t0.031514\n",
      "  validation accuracy:\t\t99.14 %%\n",
      "Epoch 146 of 200 took 173.400s\n",
      "  training loss:\t\t0.007450\n",
      "  validation loss:\t\t0.032183\n",
      "  validation accuracy:\t\t99.17 %%\n",
      "Epoch 147 of 200 took 173.457s\n",
      "  training loss:\t\t0.008543\n",
      "  validation loss:\t\t0.030296\n",
      "  validation accuracy:\t\t99.14 %%\n",
      "Epoch 148 of 200 took 173.298s\n",
      "  training loss:\t\t0.008174\n",
      "  validation loss:\t\t0.032220\n",
      "  validation accuracy:\t\t99.14 %%\n",
      "Epoch 149 of 200 took 173.503s\n",
      "  training loss:\t\t0.007685\n",
      "  validation loss:\t\t0.032013\n",
      "  validation accuracy:\t\t99.06 %%\n",
      "Epoch 150 of 200 took 173.853s\n",
      "  training loss:\t\t0.008260\n",
      "  validation loss:\t\t0.033074\n",
      "  validation accuracy:\t\t99.11 %%\n",
      "Epoch 151 of 200 took 173.243s\n",
      "  training loss:\t\t0.007655\n",
      "  validation loss:\t\t0.032096\n",
      "  validation accuracy:\t\t99.15 %%\n",
      "Epoch 152 of 200 took 173.461s\n",
      "  training loss:\t\t0.008345\n",
      "  validation loss:\t\t0.031973\n",
      "  validation accuracy:\t\t99.07 %%\n",
      "Epoch 153 of 200 took 173.656s\n",
      "  training loss:\t\t0.007511\n",
      "  validation loss:\t\t0.031457\n",
      "  validation accuracy:\t\t99.17 %%\n",
      "Epoch 154 of 200 took 173.543s\n",
      "  training loss:\t\t0.007568\n",
      "  validation loss:\t\t0.032725\n",
      "  validation accuracy:\t\t99.10 %%\n",
      "Epoch 155 of 200 took 173.361s\n",
      "  training loss:\t\t0.007542\n",
      "  validation loss:\t\t0.032427\n",
      "  validation accuracy:\t\t99.13 %%\n",
      "Epoch 156 of 200 took 176.414s\n",
      "  training loss:\t\t0.007517\n",
      "  validation loss:\t\t0.031518\n",
      "  validation accuracy:\t\t99.17 %%\n",
      "Epoch 157 of 200 took 174.121s\n",
      "  training loss:\t\t0.007363\n",
      "  validation loss:\t\t0.031342\n",
      "  validation accuracy:\t\t99.14 %%\n",
      "Epoch 158 of 200 took 173.858s\n",
      "  training loss:\t\t0.007550\n",
      "  validation loss:\t\t0.032905\n",
      "  validation accuracy:\t\t99.07 %%\n",
      "Epoch 159 of 200 took 173.612s\n",
      "  training loss:\t\t0.007767\n",
      "  validation loss:\t\t0.031707\n",
      "  validation accuracy:\t\t99.08 %%\n",
      "Epoch 160 of 200 took 173.592s\n",
      "  training loss:\t\t0.007429\n",
      "  validation loss:\t\t0.031664\n",
      "  validation accuracy:\t\t99.15 %%\n",
      "Epoch 161 of 200 took 173.539s\n",
      "  training loss:\t\t0.007225\n",
      "  validation loss:\t\t0.031905\n",
      "  validation accuracy:\t\t99.16 %%\n",
      "Epoch 162 of 200 took 174.078s\n",
      "  training loss:\t\t0.007293\n",
      "  validation loss:\t\t0.035006\n",
      "  validation accuracy:\t\t99.17 %%\n",
      "Epoch 163 of 200 took 173.878s\n",
      "  training loss:\t\t0.007529\n",
      "  validation loss:\t\t0.032926\n",
      "  validation accuracy:\t\t99.14 %%\n",
      "Epoch 164 of 200 took 173.257s\n",
      "  training loss:\t\t0.007543\n",
      "  validation loss:\t\t0.033868\n",
      "  validation accuracy:\t\t99.20 %%\n",
      "Epoch 165 of 200 took 174.006s\n",
      "  training loss:\t\t0.007537\n",
      "  validation loss:\t\t0.033564\n",
      "  validation accuracy:\t\t99.09 %%\n",
      "Epoch 166 of 200 took 173.641s\n",
      "  training loss:\t\t0.006623\n",
      "  validation loss:\t\t0.031516\n",
      "  validation accuracy:\t\t99.17 %%\n",
      "Epoch 167 of 200 took 173.759s\n",
      "  training loss:\t\t0.007161\n",
      "  validation loss:\t\t0.031353\n",
      "  validation accuracy:\t\t99.20 %%\n",
      "Epoch 168 of 200 took 173.669s\n",
      "  training loss:\t\t0.007037\n",
      "  validation loss:\t\t0.031438\n",
      "  validation accuracy:\t\t99.17 %%\n",
      "Epoch 169 of 200 took 173.841s\n",
      "  training loss:\t\t0.006709\n",
      "  validation loss:\t\t0.032151\n",
      "  validation accuracy:\t\t99.19 %%\n",
      "Epoch 170 of 200 took 173.770s\n",
      "  training loss:\t\t0.007442\n",
      "  validation loss:\t\t0.033068\n",
      "  validation accuracy:\t\t99.09 %%\n",
      "Epoch 171 of 200 took 173.268s\n",
      "  training loss:\t\t0.007050\n",
      "  validation loss:\t\t0.031315\n",
      "  validation accuracy:\t\t99.14 %%\n",
      "Epoch 172 of 200 took 173.509s\n",
      "  training loss:\t\t0.006382\n",
      "  validation loss:\t\t0.031659\n",
      "  validation accuracy:\t\t99.16 %%\n",
      "Epoch 173 of 200 took 173.764s\n",
      "  training loss:\t\t0.006387\n",
      "  validation loss:\t\t0.032373\n",
      "  validation accuracy:\t\t99.15 %%\n",
      "Epoch 174 of 200 took 173.544s\n",
      "  training loss:\t\t0.006617\n",
      "  validation loss:\t\t0.031862\n",
      "  validation accuracy:\t\t99.18 %%\n",
      "Epoch 175 of 200 took 173.413s\n",
      "  training loss:\t\t0.006631\n",
      "  validation loss:\t\t0.032079\n",
      "  validation accuracy:\t\t99.16 %%\n",
      "Epoch 176 of 200 took 173.690s\n",
      "  training loss:\t\t0.006243\n",
      "  validation loss:\t\t0.032685\n",
      "  validation accuracy:\t\t99.13 %%\n",
      "Epoch 177 of 200 took 173.864s\n",
      "  training loss:\t\t0.005994\n",
      "  validation loss:\t\t0.032110\n",
      "  validation accuracy:\t\t99.17 %%\n",
      "Epoch 178 of 200 took 173.476s\n",
      "  training loss:\t\t0.005927\n",
      "  validation loss:\t\t0.033062\n",
      "  validation accuracy:\t\t99.19 %%\n",
      "Epoch 179 of 200 took 173.957s\n",
      "  training loss:\t\t0.006503\n",
      "  validation loss:\t\t0.032676\n",
      "  validation accuracy:\t\t99.13 %%\n",
      "Epoch 180 of 200 took 176.436s\n",
      "  training loss:\t\t0.006018\n",
      "  validation loss:\t\t0.031489\n",
      "  validation accuracy:\t\t99.13 %%\n",
      "Epoch 181 of 200 took 173.747s\n",
      "  training loss:\t\t0.006394\n",
      "  validation loss:\t\t0.033384\n",
      "  validation accuracy:\t\t99.13 %%\n",
      "Epoch 182 of 200 took 173.802s\n",
      "  training loss:\t\t0.006820\n",
      "  validation loss:\t\t0.032817\n",
      "  validation accuracy:\t\t99.14 %%\n",
      "Epoch 183 of 200 took 173.908s\n",
      "  training loss:\t\t0.006171\n",
      "  validation loss:\t\t0.032586\n",
      "  validation accuracy:\t\t99.18 %%\n",
      "Epoch 184 of 200 took 173.744s\n",
      "  training loss:\t\t0.006022\n",
      "  validation loss:\t\t0.033232\n",
      "  validation accuracy:\t\t99.13 %%\n",
      "Epoch 185 of 200 took 173.798s\n",
      "  training loss:\t\t0.006279\n",
      "  validation loss:\t\t0.031606\n",
      "  validation accuracy:\t\t99.18 %%\n",
      "Epoch 186 of 200 took 173.944s\n",
      "  training loss:\t\t0.006711\n",
      "  validation loss:\t\t0.031931\n",
      "  validation accuracy:\t\t99.07 %%\n",
      "Epoch 187 of 200 took 173.707s\n",
      "  training loss:\t\t0.005398\n",
      "  validation loss:\t\t0.033390\n",
      "  validation accuracy:\t\t99.17 %%\n",
      "Epoch 188 of 200 took 173.770s\n",
      "  training loss:\t\t0.005815\n",
      "  validation loss:\t\t0.032712\n",
      "  validation accuracy:\t\t99.18 %%\n",
      "Epoch 189 of 200 took 173.708s\n",
      "  training loss:\t\t0.004947\n",
      "  validation loss:\t\t0.032752\n",
      "  validation accuracy:\t\t99.18 %%\n",
      "Epoch 190 of 200 took 173.928s\n",
      "  training loss:\t\t0.005936\n",
      "  validation loss:\t\t0.033004\n",
      "  validation accuracy:\t\t99.21 %%\n",
      "Epoch 191 of 200 took 173.967s\n",
      "  training loss:\t\t0.005618\n",
      "  validation loss:\t\t0.033511\n",
      "  validation accuracy:\t\t99.15 %%\n",
      "Epoch 192 of 200 took 173.576s\n",
      "  training loss:\t\t0.006218\n",
      "  validation loss:\t\t0.032888\n",
      "  validation accuracy:\t\t99.21 %%\n",
      "Epoch 193 of 200 took 174.078s\n",
      "  training loss:\t\t0.005733\n",
      "  validation loss:\t\t0.033180\n",
      "  validation accuracy:\t\t99.20 %%\n",
      "Epoch 194 of 200 took 173.684s\n",
      "  training loss:\t\t0.006005\n",
      "  validation loss:\t\t0.031962\n",
      "  validation accuracy:\t\t99.21 %%\n",
      "Epoch 195 of 200 took 173.845s\n",
      "  training loss:\t\t0.005988\n",
      "  validation loss:\t\t0.033247\n",
      "  validation accuracy:\t\t99.11 %%\n",
      "Epoch 196 of 200 took 173.504s\n",
      "  training loss:\t\t0.005639\n",
      "  validation loss:\t\t0.035591\n",
      "  validation accuracy:\t\t99.20 %%\n",
      "Epoch 197 of 200 took 173.544s\n",
      "  training loss:\t\t0.005416\n",
      "  validation loss:\t\t0.031794\n",
      "  validation accuracy:\t\t99.12 %%\n",
      "Epoch 198 of 200 took 173.600s\n",
      "  training loss:\t\t0.005694\n",
      "  validation loss:\t\t0.032934\n",
      "  validation accuracy:\t\t99.18 %%\n",
      "Epoch 199 of 200 took 173.856s\n",
      "  training loss:\t\t0.005443\n",
      "  validation loss:\t\t0.033474\n",
      "  validation accuracy:\t\t99.18 %%\n",
      "Epoch 200 of 200 took 173.266s\n",
      "  training loss:\t\t0.005285\n",
      "  validation loss:\t\t0.033518\n",
      "  validation accuracy:\t\t99.20 %%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dikien/anaconda/lib/python2.7/site-packages/Lasagne-0.1.dev0-py2.7.egg/lasagne/init.py:86: UserWarning: The uniform initializer no longer uses Glorot et al.'s approach to determine the bounds, but defaults to the range (-0.01, 0.01) instead. Please use the new GlorotUniform initializer to get the old behavior. GlorotUniform is now the default for all layers.\n",
      "  warnings.warn(\"The uniform initializer no longer uses Glorot et al.'s \"\n",
      "/Users/dikien/anaconda/lib/python2.7/site-packages/Lasagne-0.1.dev0-py2.7.egg/lasagne/layers/helper.py:69: UserWarning: get_all_layers() has been changed to return layers in topological order. The former implementation is still available as get_all_layers_old(), but will be removed before the first release of Lasagne. To ignore this warning, use `warnings.filterwarnings('ignore', '.*topo.*')`.\n",
      "  warnings.warn(\"get_all_layers() has been changed to return layers in \"\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "with gzip.open(DATA_FILENAME, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "dataset = load_data(data)\n",
    "\n",
    "print(\"Building model and compiling functions...\")\n",
    "output_layer = build_model(\n",
    "    input_height=dataset['input_height'],\n",
    "    input_width=dataset['input_width'],\n",
    "    output_dim=dataset['output_dim'],\n",
    "    )\n",
    "\n",
    "iter_funcs = create_iter_functions(\n",
    "    dataset,\n",
    "    output_layer,\n",
    "    X_tensor_type=T.tensor4,\n",
    "    )\n",
    "\n",
    "num_epochs = NUM_EPOCHS\n",
    "\n",
    "print(\"Starting training...\")\n",
    "now = time.time()\n",
    "try:\n",
    "    for epoch in train(iter_funcs, dataset):\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch['number'], num_epochs, time.time() - now))\n",
    "        now = time.time()\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(epoch['train_loss']))\n",
    "        print(\"  validation loss:\\t\\t{:.6f}\".format(epoch['valid_loss']))\n",
    "        print(\"  validation accuracy:\\t\\t{:.2f} %%\".format(\n",
    "            epoch['valid_accuracy'] * 100))\n",
    "\n",
    "        if epoch['number'] >= num_epochs:\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "with gzip.open(DATA_FILENAME, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "dataset = load_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset['input_height']\n",
    "# dataset['input_width']\n",
    "dataset['output_dim']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
