{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "requires a GPU to work",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ee4ba7f507c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcuda_convnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dikien/anaconda/lib/python2.7/site-packages/Lasagne-0.1.dev0-py2.7.egg/lasagne/layers/cuda_convnet.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"requires a GPU to work\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: requires a GPU to work"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import gzip\n",
    "import itertools\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import lasagne\n",
    "from lasagne.layers import cuda_convnet\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_FILENAME = 'mnist.pkl.gz'\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 600\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(data):\n",
    "    X_train, y_train = data[0]\n",
    "    X_valid, y_valid = data[1]\n",
    "    X_test, y_test = data[2]\n",
    "\n",
    "    # reshape for convolutions\n",
    "    X_train = X_train.reshape((X_train.shape[0], 1, 28, 28))\n",
    "    X_valid = X_valid.reshape((X_valid.shape[0], 1, 28, 28))\n",
    "    X_test = X_test.reshape((X_test.shape[0], 1, 28, 28))\n",
    "\n",
    "    return dict(\n",
    "        X_train=theano.shared(lasagne.utils.floatX(X_train)),\n",
    "        y_train=T.cast(theano.shared(y_train), 'int32'),\n",
    "        X_valid=theano.shared(lasagne.utils.floatX(X_valid)),\n",
    "        y_valid=T.cast(theano.shared(y_valid), 'int32'),\n",
    "        X_test=theano.shared(lasagne.utils.floatX(X_test)),\n",
    "        y_test=T.cast(theano.shared(y_test), 'int32'),\n",
    "        num_examples_train=X_train.shape[0],\n",
    "        num_examples_valid=X_valid.shape[0],\n",
    "        num_examples_test=X_test.shape[0],\n",
    "        input_height=X_train.shape[2],\n",
    "        input_width=X_train.shape[3],\n",
    "        output_dim=10,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(input_width, input_height, output_dim,\n",
    "                batch_size=BATCH_SIZE, dimshuffle=True):\n",
    "    l_in = lasagne.layers.InputLayer(\n",
    "        shape=(batch_size, 1, input_width, input_height),\n",
    "    )\n",
    "\n",
    "    if not dimshuffle:\n",
    "        l_in = cuda_convnet.bc01_to_c01b(l_in)\n",
    "\n",
    "    l_conv1 = cuda_convnet.Conv2DCCLayer(\n",
    "        l_in,\n",
    "        num_filters=32,\n",
    "        filter_size=(5, 5),\n",
    "        nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        dimshuffle=dimshuffle,\n",
    "    )\n",
    "    l_pool1 = cuda_convnet.MaxPool2DCCLayer(\n",
    "        l_conv1,\n",
    "        pool_size=(2, 2),\n",
    "        dimshuffle=dimshuffle,\n",
    "    )\n",
    "\n",
    "    l_conv2 = cuda_convnet.Conv2DCCLayer(\n",
    "        l_pool1,\n",
    "        num_filters=32,\n",
    "        filter_size=(5, 5),\n",
    "        nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        dimshuffle=dimshuffle,\n",
    "    )\n",
    "    l_pool2 = cuda_convnet.MaxPool2DCCLayer(\n",
    "        l_conv2,\n",
    "        pool_size=(2, 2),\n",
    "        dimshuffle=dimshuffle,\n",
    "    )\n",
    "\n",
    "    if not dimshuffle:\n",
    "        l_pool2 = cuda_convnet.c01b_to_bc01(l_pool2)\n",
    "\n",
    "    l_hidden1 = lasagne.layers.DenseLayer(\n",
    "        l_pool2,\n",
    "        num_units=256,\n",
    "        nonlinearity=lasagne.nonlinearities.rectify,\n",
    "    )\n",
    "\n",
    "    l_hidden1_dropout = lasagne.layers.DropoutLayer(l_hidden1, p=0.5)\n",
    "\n",
    "    # l_hidden2 = lasagne.layers.DenseLayer(\n",
    "    #     l_hidden1_dropout,\n",
    "    #     num_units=256,\n",
    "    #     nonlinearity=lasagne.nonlinearities.rectify,\n",
    "    #     )\n",
    "    # l_hidden2_dropout = lasagne.layers.DropoutLayer(l_hidden2, p=0.5)\n",
    "\n",
    "    l_out = lasagne.layers.DenseLayer(\n",
    "        l_hidden1_dropout,\n",
    "        num_units=output_dim,\n",
    "        nonlinearity=lasagne.nonlinearities.softmax,\n",
    "    )\n",
    "\n",
    "    return l_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_iter_functions(dataset, output_layer,\n",
    "                          X_tensor_type=T.matrix,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          learning_rate=LEARNING_RATE, momentum=MOMENTUM):\n",
    "    \"\"\"Create functions for training, validation and testing to iterate one\n",
    "       epoch.\n",
    "    \"\"\"\n",
    "    batch_index = T.iscalar('batch_index')\n",
    "    X_batch = X_tensor_type('x')\n",
    "    y_batch = T.ivector('y')\n",
    "    batch_slice = slice(batch_index * batch_size,\n",
    "                        (batch_index + 1) * batch_size)\n",
    "\n",
    "    objective = lasagne.objectives.Objective(output_layer,\n",
    "        loss_function=lasagne.objectives.categorical_crossentropy)\n",
    "\n",
    "    loss_train = objective.get_loss(X_batch, target=y_batch)\n",
    "    loss_eval = objective.get_loss(X_batch, target=y_batch,\n",
    "                                   deterministic=True)\n",
    "\n",
    "    pred = T.argmax(\n",
    "        lasagne.layers.get_output(output_layer, X_batch, deterministic=True),\n",
    "        axis=1)\n",
    "    accuracy = T.mean(T.eq(pred, y_batch), dtype=theano.config.floatX)\n",
    "\n",
    "    all_params = lasagne.layers.get_all_params(output_layer)\n",
    "    updates = lasagne.updates.nesterov_momentum(\n",
    "        loss_train, all_params, learning_rate, momentum)\n",
    "\n",
    "    iter_train = theano.function(\n",
    "        [batch_index], loss_train,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            X_batch: dataset['X_train'][batch_slice],\n",
    "            y_batch: dataset['y_train'][batch_slice],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    iter_valid = theano.function(\n",
    "        [batch_index], [loss_eval, accuracy],\n",
    "        givens={\n",
    "            X_batch: dataset['X_valid'][batch_slice],\n",
    "            y_batch: dataset['y_valid'][batch_slice],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    iter_test = theano.function(\n",
    "        [batch_index], [loss_eval, accuracy],\n",
    "        givens={\n",
    "            X_batch: dataset['X_test'][batch_slice],\n",
    "            y_batch: dataset['y_test'][batch_slice],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return dict(\n",
    "        train=iter_train,\n",
    "        valid=iter_valid,\n",
    "        test=iter_test,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(iter_funcs, dataset, batch_size=BATCH_SIZE):\n",
    "    \"\"\"Train the model with `dataset` with mini-batch training. Each\n",
    "       mini-batch has `batch_size` recordings.\n",
    "    \"\"\"\n",
    "    num_batches_train = dataset['num_examples_train'] // batch_size\n",
    "    num_batches_valid = dataset['num_examples_valid'] // batch_size\n",
    "\n",
    "    for epoch in itertools.count(1):\n",
    "        batch_train_losses = []\n",
    "        for b in range(num_batches_train):\n",
    "            batch_train_loss = iter_funcs['train'](b)\n",
    "            batch_train_losses.append(batch_train_loss)\n",
    "\n",
    "        avg_train_loss = np.mean(batch_train_losses)\n",
    "\n",
    "        batch_valid_losses = []\n",
    "        batch_valid_accuracies = []\n",
    "        for b in range(num_batches_valid):\n",
    "            batch_valid_loss, batch_valid_accuracy = iter_funcs['valid'](b)\n",
    "            batch_valid_losses.append(batch_valid_loss)\n",
    "            batch_valid_accuracies.append(batch_valid_accuracy)\n",
    "\n",
    "        avg_valid_loss = np.mean(batch_valid_losses)\n",
    "        avg_valid_accuracy = np.mean(batch_valid_accuracies)\n",
    "\n",
    "        yield {\n",
    "            'number': epoch,\n",
    "            'train_loss': avg_train_loss,\n",
    "            'valid_loss': avg_valid_loss,\n",
    "            'valid_accuracy': avg_valid_accuracy,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Building model and compiling functions...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'cuda_convnet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a84a16e5d426>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0minput_height\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_height'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0minput_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_width'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output_dim'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     )\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-83fe30f84aa1>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(input_width, input_height, output_dim, batch_size, dimshuffle)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0ml_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcuda_convnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbc01_to_c01b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     l_conv1 = cuda_convnet.Conv2DCCLayer(\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0ml_in\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mnum_filters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'cuda_convnet' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "with gzip.open(DATA_FILENAME, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "dataset = load_data(data)\n",
    "\n",
    "print(\"Building model and compiling functions...\")\n",
    "output_layer = build_model(\n",
    "    input_height=dataset['input_height'],\n",
    "    input_width=dataset['input_width'],\n",
    "    output_dim=dataset['output_dim'],\n",
    "    )\n",
    "\n",
    "iter_funcs = create_iter_functions(\n",
    "    dataset,\n",
    "    output_layer,\n",
    "    X_tensor_type=T.tensor4,\n",
    "    )\n",
    "\n",
    "num_epochs = NUM_EPOCHS\n",
    "\n",
    "print(\"Starting training...\")\n",
    "now = time.time()\n",
    "try:\n",
    "    for epoch in train(iter_funcs, dataset):\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch['number'], num_epochs, time.time() - now))\n",
    "        now = time.time()\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(epoch['train_loss']))\n",
    "        print(\"  validation loss:\\t\\t{:.6f}\".format(epoch['valid_loss']))\n",
    "        print(\"  validation accuracy:\\t\\t{:.2f} %%\".format(\n",
    "            epoch['valid_accuracy'] * 100))\n",
    "\n",
    "        if epoch['number'] >= num_epochs:\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
