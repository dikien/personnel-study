{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence, one_hot, Tokenizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def base_filter():\n",
    "    f = string.punctuation\n",
    "    f = f.replace(\"'\", '')\n",
    "    f += '\\t\\n'\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = \"i am a boy\\\n",
    "\\nyou are boy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'a', 'boy', 'you', 'are', 'boy']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_word_sequence(text, filters=base_filter(), lower=True, split=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 2, 3, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "t = one_hot(text, n=5, filters=base_filter(), lower=True, split=\" \")\n",
    "print t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(nb_words=10, filters=base_filter(), lower=True, split=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tokenizer.fit_on_texts(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[1,2, 6], [1,2, 5]]\n",
    "tokenizer.sequences_to_matrix(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n",
      "\n",
      "None\n",
      "{'a': 3, ' ': 5, 'b': 2, 'e': 1, 'i': 1, '\\n': 1, 'm': 1, 'o': 3, 'r': 1, 'u': 1, 'y': 3}\n",
      "None\n",
      "{'a': 3, 'b': 2, 'e': 1, 'i': 1, 'm': 1, 'o': 3, 'r': 1, 'u': 1, 'y': 3}\n",
      "{'a': 3, 'b': 2, 'e': 1, 'i': 1, 'm': 1, 'o': 3, 'r': 1, 'u': 1, 'y': 3}\n",
      "{'a': 1, 'b': 4, 'e': 5, 'i': 6, 'm': 7, 'o': 2, 'r': 8, 'u': 9, 'y': 3}\n"
     ]
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(text)\n",
    "print tokenizer.document_count\n",
    "print tokenizer.filters\n",
    "print tokenizer.fit_on_sequences(text)\n",
    "print tokenizer.index_docs\n",
    "print tokenizer.nb_words\n",
    "print tokenizer.word_counts\n",
    "print tokenizer.word_docs\n",
    "print tokenizer.word_index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
