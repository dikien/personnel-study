{"nbformat_minor": 0, "cells": [{"execution_count": 1, "cell_type": "code", "source": "import numpy as np\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nimport re\nfrom nltk.stem.porter import *\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction import text\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import decomposition, pipeline, metrics, grid_search", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 2, "cell_type": "code", "source": "# The following 3 functions have been taken from Ben Hamner's github repository\n# https://github.com/benhamner/Metrics\ndef confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the confusion matrix between rater's ratings\n    \"\"\"\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_ratings = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_ratings)]\n                for j in range(num_ratings)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the counts of each type of rating that a rater made\n    \"\"\"\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\n\ndef quadratic_weighted_kappa(y, y_pred):\n    \"\"\"\n    Calculates the quadratic weighted kappa\n    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n    value, which is a measure of inter-rater agreement between two raters\n    that provide discrete numeric ratings.  Potential values range from -1\n    (representing complete disagreement) to 1 (representing complete\n    agreement).  A kappa value of 0 is expected if all agreement is due to\n    chance.\n    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n    each correspond to a list of integer ratings.  These lists must have the\n    same length.\n    The ratings should be integers, and it is assumed that they contain\n    the complete range of possible ratings.\n    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n    is the minimum possible rating, and max_rating is the maximum possible\n    rating\n    \"\"\"\n    rater_a = y\n    rater_b = y_pred\n    min_rating=None\n    max_rating=None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(min(rater_a), min(rater_b))\n    if max_rating is None:\n        max_rating = max(max(rater_a), max(rater_b))\n    conf_mat = confusion_matrix(rater_a, rater_b,\n                                min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n                              / num_scored_items)\n            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] / num_scored_items\n            denominator += d * expected_count / num_scored_items\n\n    return (1.0 - numerator / denominator)\n\n# Kappa Scorer \nkappa_scorer = metrics.make_scorer(quadratic_weighted_kappa, greater_is_better = True)", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"execution_count": 3, "cell_type": "code", "source": "# Load the training file\ntrain = pd.read_csv('train.csv')\ntest = pd.read_csv('test.csv')\n\n# we dont need ID columns\nidx = test.id.values.astype(int)\ntrain = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)\n\n# non values\ub97c \" \"\uc73c\ub85c \ucc44\uc6b0\uae30\ntrain = train.fillna(\" \")\ntest  = test.fillna(\" \")\n\n#remove html, remove non text or numeric, make query and title unique features for counts using prefix (accounted for in stopwords tweak)\nstemmer = PorterStemmer()\n\n## Stemming functionality\nclass stemmerUtility(object):\n    \"\"\"Stemming functionality\"\"\"\n    @staticmethod\n    def stemPorter(review_text):\n        porter = PorterStemmer()\n        preprocessed_docs = []\n        for doc in review_text:\n            final_doc = []\n            for word in doc:\n                final_doc.append(porter.stem(word))\n                #final_doc.append(wordnet.lemmatize(word)) #note that lemmatize() can also takes part of speech as an argument!\n            preprocessed_docs.append(final_doc)\n        return preprocessed_docs\n\ndef clean(text):\n    text = BeautifulSoup(text).get_text(\" \")\n    text = re.sub(\"[^a-zA-Z0-9]\",\" \", text)\n    text = (\" \").join([stemmer.stem(z) for z in text.split()])\n    return text\n\ndef cleanq(text):\n    text = BeautifulSoup(text).get_text(\" \")\n    text = re.sub(\"[^a-zA-Z0-9]\",\" \", text)\n    text = (\" \").join([\"q\" + stemmer.stem(z) for z in text.split()])\n    return text\n\ndef cleant(text):\n    text = BeautifulSoup(text).get_text(\" \")\n    text = re.sub(\"[^a-zA-Z0-9]\",\" \", text)\n    text = (\" \").join([\"t\" + stemmer.stem(z) for z in text.split()])\n    return text\n\n# clean data\ntrain['query'] = train['query'].apply(func=cleanq)\ntrain['product_title'] = train['product_title'].apply(func=cleant)\ntrain['product_description'] = train['product_description'].apply(func=clean)\n\ntest['query'] = test['query'].apply(func=cleanq)\ntest['product_title'] = test['product_title'].apply(func=cleant)\ntest['product_description'] = test['product_description'].apply(func=clean)", "outputs": [{"output_type": "stream", "name": "stderr", "text": "/home/ubuntu/anaconda/lib/python2.7/site-packages/bs4/__init__.py:189: UserWarning: \"http://i104.photobucket.com/albums/m175/champions_on_display/wincraft2013/januaryb/65497012.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n/home/ubuntu/anaconda/lib/python2.7/site-packages/bs4/__init__.py:189: UserWarning: \"http://i104.photobucket.com/albums/m175/champions_on_display/wincraft2013/januaryb/65516012.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n/home/ubuntu/anaconda/lib/python2.7/site-packages/bs4/__init__.py:189: UserWarning: \"http://i104.photobucket.com/albums/m175/champions_on_display/wincraft2013/januaryb/6552101\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n/home/ubuntu/anaconda/lib/python2.7/site-packages/bs4/__init__.py:189: UserWarning: \"http://i104.photobucket.com/albums/m175/champions_on_display/wincraft2013/januaryb/65527\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n/home/ubuntu/anaconda/lib/python2.7/site-packages/bs4/__init__.py:189: UserWarning: \"http://i104.photobucket.com/albums/m175/champions_on_display/wincraft2013/januarya/14146012.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 4, "cell_type": "code", "source": "def merge_rows(x):\n    query = x[0]\n    product_title = x[1]\n    product_description  = x[2]\n    return query + ' ' + product_title + ' ' + product_description\n\ntrainX = train[['query', 'product_title', 'product_description']].apply(func=merge_rows, axis=1)\ntrainY = train[\"median_relevance\"]\n\ntestX = test[['query', 'product_title', 'product_description']].apply(func=merge_rows, axis=1)", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"execution_count": 5, "cell_type": "code", "source": "# the infamous tfidf vectorizer (Do you remember this one?)\ntfv = TfidfVectorizer(min_df=3,  max_features=None, max_df=500,\n        strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n        ngram_range=(1, 2), use_idf=1, smooth_idf=1, sublinear_tf=1,\n        stop_words = 'english')\n\n# Fit TFIDF\ntfv.fit(trainX)\ntrainX =  tfv.transform(trainX) \ntestX = tfv.transform(testX)", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"execution_count": 6, "cell_type": "code", "source": "# Initialize SVD\nsvd = TruncatedSVD(algorithm='randomized', n_iter=5, random_state=None, tol=0.0)\n\n# Initialize the standard scaler \nscl = StandardScaler(copy=True, with_mean=True, with_std=True)\n\n# We will use SVM here..\nsvm_model = SVC(kernel='rbf', degree=3, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=4000, class_weight=None, verbose=False, max_iter=-1, random_state=None)\n\n# Create the pipeline \nclf = pipeline.Pipeline([('svd', svd),\n                         ('scl', scl),\n                         ('svm', svm_model)])\n\n# Create a parameter grid to search for best parameters for everything in the pipeline\nparam_grid = {'svd__n_components' : [100, 200, 300, 400, 500, 600],\n              'svm__C': [10]}", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 7, "cell_type": "code", "source": "# Initialize Grid Search Model\nmodel = grid_search.GridSearchCV(estimator = clf, param_grid=param_grid, scoring=kappa_scorer,\n                                 verbose=2, n_jobs=6, iid=True, refit=True, cv=2)", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 8, "cell_type": "code", "source": "# Fit Grid Search Model\nmodel.fit(trainX, trainY)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n"}, {"output_type": "stream", "name": "stderr", "text": "[Parallel(n_jobs=6)]: Done   1 out of  12 | elapsed:   21.1s remaining:  3.9min\n[Parallel(n_jobs=6)]: Done   9 out of  12 | elapsed:  2.4min remaining:   47.5s\n[Parallel(n_jobs=6)]: Done  12 out of  12 | elapsed:  3.1min finished\n"}, {"output_type": "stream", "name": "stdout", "text": "[CV] svm__C=10, svd__n_components=100 ................................\n[CV] svm__C=10, svd__n_components=100 ................................\n[CV] svm__C=10, svd__n_components=200 ................................\n[CV] svm__C=10, svd__n_components=200 ................................\n[CV] svm__C=10, svd__n_components=300 ................................\n[CV] svm__C=10, svd__n_components=300 ................................\n[CV] ....................... svm__C=10, svd__n_components=100 -  21.0s[CV] ....................... svm__C=10, svd__n_components=100 -  20.2s[CV] ....................... svm__C=10, svd__n_components=200 -  41.0s[CV] ....................... svm__C=10, svd__n_components=200 -  39.5s[CV] ....................... svm__C=10, svd__n_components=300 - 1.1min[CV] ....................... svm__C=10, svd__n_components=300 - 1.1min\n\n\n\n\n\n[CV] svm__C=10, svd__n_components=400 ................................\n[CV] svm__C=10, svd__n_components=400 ................................\n[CV] svm__C=10, svd__n_components=500 ................................\n[CV] svm__C=10, svd__n_components=500 ................................\n[CV] svm__C=10, svd__n_components=600 ................................\n[CV] svm__C=10, svd__n_components=600 ................................\n[CV] ....................... svm__C=10, svd__n_components=400 - 1.7min[CV] ....................... svm__C=10, svd__n_components=400 - 1.6min[CV] ....................... svm__C=10, svd__n_components=500 - 1.8min[CV] ....................... svm__C=10, svd__n_components=500 - 1.7min[CV] ....................... svm__C=10, svd__n_components=600 - 2.0min[CV] ....................... svm__C=10, svd__n_components=600 - 2.0min\n\n\n\n\n\n"}, {"execution_count": 8, "output_type": "execute_result", "data": {"text/plain": "GridSearchCV(cv=2,\n       estimator=Pipeline(steps=[('svd', TruncatedSVD(algorithm='randomized', n_components=2, n_iter=5,\n       random_state=None, tol=0.0)), ('scl', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm', SVC(C=1.0, cache_size=4000, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n  shrinking=True, tol=0.001, verbose=False))]),\n       fit_params={}, iid=True, loss_func=None, n_jobs=6,\n       param_grid={'svm__C': [10], 'svd__n_components': [100, 200, 300, 400, 500, 600]},\n       pre_dispatch='2*n_jobs', refit=True, score_func=None,\n       scoring=make_scorer(quadratic_weighted_kappa), verbose=2)"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 9, "cell_type": "code", "source": "print(\"Best score: %0.3f\" % model.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = model.best_estimator_.get_params()\nfor param_name in sorted(param_grid.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Best score: 0.528\nBest parameters set:\n\tsvd__n_components: 200\n\tsvm__C: 10\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 10, "cell_type": "code", "source": "# 2\ubc88\uc9f8 \uc2dc\ub3c4", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"execution_count": 11, "cell_type": "code", "source": "# Initialize SVD\nsvd = TruncatedSVD(algorithm='randomized', n_iter=5, random_state=None, tol=0.0)\n\n# Initialize the standard scaler \nscl = StandardScaler(copy=True, with_mean=True, with_std=True)\n\n# We will use SVM here..\nsvm_model = SVC(kernel='rbf', degree=3, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=4000, class_weight=None, verbose=False, max_iter=-1, random_state=None)\n\n# Create the pipeline \nclf = pipeline.Pipeline([('svd', svd),\n                         ('scl', scl),\n                         ('svm', svm_model)])\n\n# Create a parameter grid to search for best parameters for everything in the pipeline\nparam_grid = {'svd__n_components' : [200, 1000, 1500, 2000, 2500, 3000],\n              'svm__C': [10]}\n\n# Initialize Grid Search Model\nmodel = grid_search.GridSearchCV(estimator = clf, param_grid=param_grid, scoring=kappa_scorer,\n                                 verbose=1, n_jobs=6, iid=True, refit=True, cv=2)\n\n# Fit Grid Search Model\nmodel.fit(trainX, trainY)\n\nprint(\"Best score: %0.3f\" % model.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = model.best_estimator_.get_params()\nfor param_name in sorted(param_grid.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n"}, {"output_type": "stream", "name": "stderr", "text": "[Parallel(n_jobs=6)]: Done   1 out of  12 | elapsed:   41.4s remaining:  7.6min\n[Parallel(n_jobs=6)]: Done  12 out of  12 | elapsed: 23.5min finished\n"}, {"output_type": "stream", "name": "stdout", "text": "Best score: 0.526\nBest parameters set:\n\tsvd__n_components: 200\n\tsvm__C: 10\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 12, "cell_type": "code", "source": "# svd__n_components: 200 \uc774\uac8c \uc81c\uc77c \ub098\uc544 \ubcf4\uc778\ub2e4\n# \uc774\uc81c svm__C\ud30c\ub77c\ubbf8\ud130\ub97c \ucd5c\uc801\ud654\ud574\ubcf4\uc790\n\n# Initialize SVD\nsvd = TruncatedSVD(algorithm='randomized', n_iter=5, random_state=None, tol=0.0)\n\n# Initialize the standard scaler \nscl = StandardScaler(copy=True, with_mean=True, with_std=True)\n\n# We will use SVM here..\nsvm_model = SVC(kernel='rbf', degree=3, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=4000, class_weight=None, verbose=False, max_iter=-1, random_state=None)\n\n# Create the pipeline \nclf = pipeline.Pipeline([('svd', svd),\n                         ('scl', scl),\n                         ('svm', svm_model)])\n\n# Create a parameter grid to search for best parameters for everything in the pipeline\nparam_grid = {'svd__n_components' : [200],\n              'svm__C': [10,20,30,40,50,60]}\n\n# Initialize Grid Search Model\nmodel = grid_search.GridSearchCV(estimator = clf, param_grid=param_grid, scoring=kappa_scorer,\n                                 verbose=1, n_jobs=6, iid=True, refit=True, cv=2)\n\n# Fit Grid Search Model\nmodel.fit(trainX, trainY)\n\nprint(\"Best score: %0.3f\" % model.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = model.best_estimator_.get_params()\nfor param_name in sorted(param_grid.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n"}, {"output_type": "stream", "name": "stderr", "text": "[Parallel(n_jobs=6)]: Done   1 out of  12 | elapsed:   47.2s remaining:  8.6min\n[Parallel(n_jobs=6)]: Done  12 out of  12 | elapsed:  1.7min finished\n"}, {"output_type": "stream", "name": "stdout", "text": "Best score: 0.535\nBest parameters set:\n\tsvd__n_components: 200\n\tsvm__C: 10\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 13, "cell_type": "code", "source": "# \uc774\uc81c svm__C\ud30c\ub77c\ubbf8\ud130\ub97c \ucd5c\uc801\ud654\ud574\ubcf4\uc790\n\n# Initialize SVD\nsvd = TruncatedSVD(algorithm='randomized', n_iter=5, random_state=None, tol=0.0)\n\n# Initialize the standard scaler \nscl = StandardScaler(copy=True, with_mean=True, with_std=True)\n\n# We will use SVM here..\nsvm_model = SVC(kernel='rbf', degree=3, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=4000, class_weight=None, verbose=False, max_iter=-1, random_state=None)\n\n# Create the pipeline \nclf = pipeline.Pipeline([('svd', svd),\n                         ('scl', scl),\n                         ('svm', svm_model)])\n\n# Create a parameter grid to search for best parameters for everything in the pipeline\nparam_grid = {'svd__n_components' : [200],\n              'svm__C': [10, 100, 200, 300, 400, 500]}\n\n# Initialize Grid Search Model\nmodel = grid_search.GridSearchCV(estimator = clf, param_grid=param_grid, scoring=kappa_scorer,\n                                 verbose=1, n_jobs=6, iid=True, refit=True, cv=2)\n\n# Fit Grid Search Model\nmodel.fit(trainX, trainY)\n\nprint(\"Best score: %0.3f\" % model.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = model.best_estimator_.get_params()\nfor param_name in sorted(param_grid.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n"}, {"output_type": "stream", "name": "stderr", "text": "[Parallel(n_jobs=6)]: Done   1 jobs       | elapsed:   46.5s\n[Parallel(n_jobs=6)]: Done   2 out of  12 | elapsed:   48.3s remaining:  4.0min\n[Parallel(n_jobs=6)]: Done  12 out of  12 | elapsed:  1.7min finished\n"}, {"output_type": "stream", "name": "stdout", "text": "Best score: 0.536\nBest parameters set:\n\tsvd__n_components: 200\n\tsvm__C: 10\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 14, "cell_type": "code", "source": "# \uc774\uc81c svm__C\ud30c\ub77c\ubbf8\ud130\ub97c \ucd5c\uc801\ud654\ud574\ubcf4\uc790\n\n# Initialize SVD\nsvd = TruncatedSVD(algorithm='randomized', n_iter=5, random_state=None, tol=0.0, n_components=200)\n\n# Initialize the standard scaler \nscl = StandardScaler(copy=True, with_mean=True, with_std=True)\n\n# We will use SVM here..\nsvm_model = SVC(kernel='rbf', degree=3, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=4000, class_weight=None, verbose=False, max_iter=-1, random_state=None)\n\n# Create the pipeline \nclf = pipeline.Pipeline([('svd', svd),\n                         ('scl', scl),\n                         ('svm', svm_model)])\n\n# Create a parameter grid to search for best parameters for everything in the pipeline\nparam_grid = {'svm__C': [1,2,3,4,5,6,7,8,9,10]}\n\n# Initialize Grid Search Model\nmodel = grid_search.GridSearchCV(estimator = clf, param_grid=param_grid, scoring=kappa_scorer,\n                                 verbose=1, n_jobs=6, iid=True, refit=True, cv=2)\n\n# Fit Grid Search Model\nmodel.fit(trainX, trainY)\n\nprint(\"Best score: %0.3f\" % model.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = model.best_estimator_.get_params()\nfor param_name in sorted(param_grid.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n"}, {"output_type": "stream", "name": "stderr", "text": "[Parallel(n_jobs=6)]: Done   1 jobs       | elapsed:   46.8s\n[Parallel(n_jobs=6)]: Done  10 out of  20 | elapsed:  1.6min remaining:  1.6min\n[Parallel(n_jobs=6)]: Done  20 out of  20 | elapsed:  2.7min finished\n"}, {"output_type": "stream", "name": "stdout", "text": "Best score: 0.535\nBest parameters set:\n\tsvm__C: 9\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 16, "cell_type": "code", "source": "# \uc774\uc81c svm__C\ud30c\ub77c\ubbf8\ud130\ub97c \ucd5c\uc801\ud654\ud574\ubcf4\uc790\n\n# Initialize SVD\nsvd = TruncatedSVD(algorithm='randomized', n_iter=5, random_state=None, tol=0.0, n_components=200)\n\n# Initialize the standard scaler \nscl = StandardScaler(copy=True, with_mean=True, with_std=True)\n\n# We will use SVM here..\nsvm_model = SVC(kernel='rbf', degree=3, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=4000, class_weight=None, verbose=False, max_iter=-1, random_state=None)\n\n# Create the pipeline \nclf = pipeline.Pipeline([('svd', svd),\n                         ('scl', scl),\n                         ('svm', svm_model)])\n\n# Create a parameter grid to search for best parameters for everything in the pipeline\nparam_grid = {'svm__C': [9.1, 9.2, 9.3, 9.4, 9.5, 9.6, 9.7, 9.8, 9.9]}\n\n# Initialize Grid Search Model\nmodel = grid_search.GridSearchCV(estimator = clf, param_grid=param_grid, scoring=kappa_scorer,\n                                 verbose=1, n_jobs=6, iid=True, refit=True, cv=2)\n\n# Fit Grid Search Model\nmodel.fit(trainX, trainY)\n\nprint(\"Best score: %0.3f\" % model.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = model.best_estimator_.get_params()\nfor param_name in sorted(param_grid.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"}, {"output_type": "stream", "name": "stderr", "text": "[Parallel(n_jobs=6)]: Done   1 jobs       | elapsed:   44.6s\n[Parallel(n_jobs=6)]: Done   8 out of  18 | elapsed:  1.5min remaining:  1.9min\n[Parallel(n_jobs=6)]: Done  18 out of  18 | elapsed:  2.2min finished\n"}, {"output_type": "stream", "name": "stdout", "text": "Best score: 0.536\nBest parameters set:\n\tsvm__C: 9.6\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 18, "cell_type": "code", "source": "# \uc774\uc81c svm__C\ud30c\ub77c\ubbf8\ud130\ub97c \ucd5c\uc801\ud654\ud574\ubcf4\uc790\n\n# Initialize SVD\nsvd = TruncatedSVD(algorithm='randomized', n_iter=5, random_state=None, tol=0.0, n_components=200)\n\n# Initialize the standard scaler \nscl = StandardScaler(copy=True, with_mean=True, with_std=True)\n\n# We will use SVM here..\nsvm_model = SVC(kernel='rbf', degree=3, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=4000, class_weight=None, verbose=False, max_iter=-1, random_state=None)\n\n# Create the pipeline \nclf = pipeline.Pipeline([('svd', svd),\n                         ('scl', scl),\n                         ('svm', svm_model)])\n\n# Create a parameter grid to search for best parameters for everything in the pipeline\nparam_grid = {'svm__C': np.random.uniform(low=9.5, high=9.7, size=30)}\n\n# Initialize Grid Search Model\nmodel = grid_search.GridSearchCV(estimator = clf, param_grid=param_grid, scoring=kappa_scorer,\n                                 verbose=1, n_jobs=6, iid=True, refit=True, cv=2)\n\n# Fit Grid Search Model\nmodel.fit(trainX, trainY)\n\nprint(\"Best score: %0.3f\" % model.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = model.best_estimator_.get_params()\nfor param_name in sorted(param_grid.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Fitting 2 folds for each of 30 candidates, totalling 60 fits\n"}, {"output_type": "stream", "name": "stderr", "text": "[Parallel(n_jobs=6)]: Done   1 jobs       | elapsed:   44.4s\n[Parallel(n_jobs=6)]: Done  50 out of  60 | elapsed:  6.3min remaining:  1.3min\n[Parallel(n_jobs=6)]: Done  60 out of  60 | elapsed:  7.1min finished\n"}, {"output_type": "stream", "name": "stdout", "text": "Best score: 0.542\nBest parameters set:\n\tsvm__C: 9.6149819013819808\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 19, "cell_type": "code", "source": "# svm__gamma\ud30c\ub77c\ubbf8\ud130\ub97c \ucd5c\uc801\ud654\ud574\ubcf4\uc790\n\n# Initialize SVD\nsvd = TruncatedSVD(algorithm='randomized', n_iter=5, random_state=None, tol=0.0, n_components=200)\n\n# Initialize the standard scaler \nscl = StandardScaler(copy=True, with_mean=True, with_std=True)\n\n# We will use SVM here..\nsvm_model = SVC(kernel='rbf', C=9.6149819013819808, degree=3, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=4000, class_weight=None, verbose=False, max_iter=-1, random_state=None)\n\n# Create the pipeline \nclf = pipeline.Pipeline([('svd', svd),\n                         ('scl', scl),\n                         ('svm', svm_model)])\n\n# Create a parameter grid to search for best parameters for everything in the pipeline\nparam_grid = {'svm__gamma': [0.0, 0.1, 0.01, 0.001, 0.0001, 0.5, 0.05, 0.005, 0.0005, 0.3, 0.03, 0.003, 0.0003\n                            ,0.7, 0.07, 0.007, 0.0007]}\n\n# Initialize Grid Search Model\nmodel = grid_search.GridSearchCV(estimator = clf, param_grid=param_grid, scoring=kappa_scorer,\n                                 verbose=1, n_jobs=6, iid=True, refit=True, cv=2)\n\n# Fit Grid Search Model\nmodel.fit(trainX, trainY)\n\nprint(\"Best score: %0.3f\" % model.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = model.best_estimator_.get_params()\nfor param_name in sorted(param_grid.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Fitting 2 folds for each of 17 candidates, totalling 34 fits\n"}, {"output_type": "stream", "name": "stderr", "text": "[Parallel(n_jobs=6)]: Done   1 jobs       | elapsed:   46.2s\n[Parallel(n_jobs=6)]: Done  34 out of  34 | elapsed:  4.9min finished\n"}, {"output_type": "stream", "name": "stdout", "text": "Best score: 0.536\nBest parameters set:\n\tsvm__gamma: 0.007\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 20, "cell_type": "code", "source": "# svm__gamma\ud30c\ub77c\ubbf8\ud130\ub97c \ucd5c\uc801\ud654\ud574\ubcf4\uc790\n\n# Initialize SVD\nsvd = TruncatedSVD(algorithm='randomized', n_iter=5, random_state=None, tol=0.0, n_components=200)\n\n# Initialize the standard scaler \nscl = StandardScaler(copy=True, with_mean=True, with_std=True)\n\n# We will use SVM here..\nsvm_model = SVC(kernel='rbf', C=9.6149819013819808, degree=3, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=4000, class_weight=None, verbose=False, max_iter=-1, random_state=None)\n\n# Create the pipeline \nclf = pipeline.Pipeline([('svd', svd),\n                         ('scl', scl),\n                         ('svm', svm_model)])\n\n# Create a parameter grid to search for best parameters for everything in the pipeline\nparam_grid = {'svm__gamma': np.random.uniform(low=0.006, high=0.008, size=100)}\n\n# Initialize Grid Search Model\nmodel = grid_search.GridSearchCV(estimator = clf, param_grid=param_grid, scoring=kappa_scorer,\n                                 verbose=1, n_jobs=6, iid=True, refit=True, cv=2)\n\n# Fit Grid Search Model\nmodel.fit(trainX, trainY)\n\nprint(\"Best score: %0.3f\" % model.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = model.best_estimator_.get_params()\nfor param_name in sorted(param_grid.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n"}, {"output_type": "stream", "name": "stderr", "text": "[Parallel(n_jobs=6)]: Done   1 jobs       | elapsed:   48.6s\n[Parallel(n_jobs=6)]: Done  50 jobs       | elapsed:  6.7min\n[Parallel(n_jobs=6)]: Done 200 out of 200 | elapsed: 24.5min finished\n"}, {"output_type": "stream", "name": "stdout", "text": "Best score: 0.544\nBest parameters set:\n\tsvm__gamma: 0.0063585387028907208\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 21, "cell_type": "code", "source": "# Get best model\nbest_model = model.best_estimator_\n\n# Fit model with best parameters optimized for quadratic_weighted_kappa\nbest_model.fit(trainX, trainY)\npreds = best_model.predict(testX)", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 30, "cell_type": "code", "source": "# Create your submission file\nsubmission = pd.DataFrame({\"id\": idx, \"prediction\": preds})\nsubmission.to_csv(\"submission.csv\", index=False)", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"execution_count": 29, "cell_type": "code", "source": "# kaggle score : 0.59655", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": 31, "cell_type": "code", "source": "# 2\uac1c\uc758 \uacb0\uacfc\uac12\uc744 \ud569\uccd0\uc11c \ud3c9\uade0\uc744 \ub0b4\ubcf4\uc790!\n# 0.61139\ns_data = train[['query', 'product_title', 'product_description']].apply(func=merge_rows, axis=1)\ns_labels = train[\"median_relevance\"]\n\nt_data = test[['query', 'product_title', 'product_description']].apply(func=merge_rows, axis=1)\n\n#create sklearn pipeline, fit all, and predit test data\nclf = Pipeline([('v',TfidfVectorizer(min_df=5, max_df=500, max_features=None, strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1, 2), use_idf=True, smooth_idf=True, sublinear_tf=True, stop_words = 'english')), \n('svd', TruncatedSVD(n_components=200, algorithm='randomized', n_iter=5, random_state=None, tol=0.0)), \n('scl', StandardScaler(copy=True, with_mean=True, with_std=True)), \n('svm', SVC(C=10.0, kernel='rbf', degree=3, gamma=0.0, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=1000, class_weight=None, verbose=False, max_iter=-1, random_state=None))])\nclf.fit(s_data, s_labels)\nt_labels = clf.predict(t_data)\n\nimport math\np3 = []\nfor i in range(len(preds)):\n    x = (int(t_labels[i]) + preds[i])/2\n    x = math.floor(x)\n    p3.append(int(x))\n    \nsubmission = pd.DataFrame({\"id\": idx, \"prediction\": p3})\nsubmission.to_csv(\"submission.csv\", index=False)", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.9", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}