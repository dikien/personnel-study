{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "species_map = {'CULEX RESTUANS' : \"100000\",\n",
    "              'CULEX TERRITANS' : \"010000\", \n",
    "              'CULEX PIPIENS'   : \"001000\", \n",
    "              'CULEX PIPIENS/RESTUANS' : \"101000\", \n",
    "              'CULEX ERRATICUS' : \"000100\", \n",
    "              'CULEX SALINARIUS': \"000010\", \n",
    "              'CULEX TARSALIS' :  \"000001\",\n",
    "              'UNSPECIFIED CULEX': \"001000\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def closest_station(x):\n",
    "    lat, longi = x[0], x[1]\n",
    "    # Chicago is small enough that we can treat coordinates as rectangular.\n",
    "    stations = np.array([[41.995, -87.933],\n",
    "                         [41.786, -87.752]])\n",
    "    loc = np.array([lat, longi])\n",
    "    deltas = stations - loc[None, :]\n",
    "    dist2 = (deltas**2).sum(1)\n",
    "    return np.argmin(dist2) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale(X, eps = 0.001):\n",
    "    # scale the data points s.t the columns of the feature space\n",
    "    # (i.e the predictors) are within the range [0, 1]\n",
    "    return (X - np.min(X, axis = 0)) / (np.max(X, axis = 0) + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(train, weather, random_state):\n",
    "    \n",
    "    # weather 정보 불러오기\n",
    "    df_weather = pd.read_csv(weather)\n",
    "    \n",
    "    # 필요한 컬럼만 추출\n",
    "    df_weather = df_weather[['Date', 'Station', 'Tmax', 'Tmin', 'Tavg', 'DewPoint', 'WetBulb', 'StnPressure']]\n",
    "    \n",
    "    # train 정보 불러오기\n",
    "    df_train = pd.read_csv(train)\n",
    "    # 필요한 컬럼만 추출\n",
    "    df_train = df_train[['Date', 'Species', 'Latitude', 'Longitude', 'NumMosquitos', 'WnvPresent']]\n",
    "    # 년 -> 월, 주 컬럼 추가\n",
    "    df_train['month'] = pd.DatetimeIndex(df_train['Date']).month\n",
    "    df_train['week'] = pd.DatetimeIndex(df_train['Date']).week\n",
    "    \n",
    "    # Species가 문자열이라서 숫자로 변경\n",
    "    df_train['Species'] = df_train['Species'].map(lambda x : species_map[x])\n",
    "    \n",
    "    # 위도 경도를 기준으로 가까운 스테이션을 찾기 위해 Station 컬럼을 추가\n",
    "    df_train['Station'] = df_train[['Latitude', 'Longitude']].apply(func=closest_station, axis=1)\n",
    "    \n",
    "    # Date, Station 을 기준으로 두개의 데이터 통합\n",
    "    df = df_train.merge(df_weather, how='inner', on=['Date', 'Station'])\n",
    "    \n",
    "    # 값들을 숫자값으로 변경\n",
    "    features = df[['Species', 'Latitude', 'Longitude', 'NumMosquitos', 'month', 'week',\n",
    "           'Tmax', 'Tmin', 'Tavg', 'Tavg', 'DewPoint', 'WetBulb', 'StnPressure']].convert_objects(convert_numeric=True)\n",
    "    \n",
    "    # null값을 그 필드의 중간값으로 변경\n",
    "    m = features['WetBulb'].median()\n",
    "    features['WetBulb'] = features['WetBulb'].fillna(m)\n",
    "    \n",
    "    m = features['StnPressure'].median()\n",
    "    features['StnPressure'] = features['StnPressure'].fillna(m)\n",
    "    \n",
    "    # data type변경\n",
    "    features = features.values.astype(\"float32\")\n",
    "    \n",
    "    # 스케일을 [0, 1]로 변경\n",
    "    features = scale(features)\n",
    "    \n",
    "    # label을 추가\n",
    "    labels = df['WnvPresent'].astype(np.int).values.reshape(features.shape[0], 1)\n",
    "    \n",
    "    features, labels = shuffle(features, labels, random_state=random_state)\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "#     labels = np_utils.to_categorical(labels)    \n",
    "#     scaler = StandardScaler()\n",
    "#     scaler.fit(features)\n",
    "#     features = scaler.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 위를 바탕으로 함수를 만들어 보자.\n",
    "\n",
    "def preprocess_test(train, weather):\n",
    "    \n",
    "    # weather 정보 불러오기\n",
    "    df_weather = pd.read_csv(weather)\n",
    "    \n",
    "    # 필요한 컬럼만 추출\n",
    "    df_weather = df_weather[['Date', 'Station', 'Tmax', 'Tmin', 'Tavg', 'DewPoint', 'WetBulb', 'StnPressure']]\n",
    "    \n",
    "    # train 정보 불러오기\n",
    "    df_train = pd.read_csv(train)\n",
    "    # 필요한 컬럼만 추출\n",
    "    df_train = df_train[['Date', 'Species', 'Latitude', 'Longitude', 'Id']]\n",
    "    # 년 -> 월, 주 컬럼 추가\n",
    "    df_train['month'] = pd.DatetimeIndex(df_train['Date']).month\n",
    "    df_train['week'] = pd.DatetimeIndex(df_train['Date']).week\n",
    "    \n",
    "    # Species가 문자열이라서 숫자로 변경\n",
    "    df_train['Species'] = df_train['Species'].map(lambda x : species_map[x])\n",
    "    \n",
    "    # 위도 경도를 기준으로 가까운 스테이션을 찾기 위해 Station 컬럼을 추가\n",
    "    df_train['Station'] = df_train[['Latitude', 'Longitude']].apply(func=closest_station, axis=1)\n",
    "    \n",
    "    # Date, Station 을 기준으로 두개의 데이터 통합\n",
    "    df = df_train.merge(df_weather, how='inner', on=['Date', 'Station'])\n",
    "    \n",
    "    # 값들을 숫자값으로 변경\n",
    "    features = df[['Species', 'Latitude', 'Longitude', 'Id', 'month', 'week',\n",
    "           'Tmax', 'Tmin', 'Tavg', 'Tavg', 'DewPoint', 'WetBulb', 'StnPressure']].convert_objects(convert_numeric=True)\n",
    "    \n",
    "    # null값을 그 필드의 중간값으로 변경\n",
    "    m = features['WetBulb'].median()\n",
    "    features['WetBulb'] = features['WetBulb'].fillna(m)\n",
    "    \n",
    "    m = features['StnPressure'].median()\n",
    "    features['StnPressure'] = features['StnPressure'].fillna(m)\n",
    "    \n",
    "    # data type변경\n",
    "    features = features.values.astype(\"float32\")\n",
    "    \n",
    "    # 스케일을 [0, 1]로 변경\n",
    "    features = scale(features)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_state = 1111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features, labels = preprocess(\"/Users/dikien/Downloads/West Nile Virus Prediction/train.csv\",\n",
    "                            \"/Users/dikien/Downloads/West Nile Virus Prediction/weather.csv\",\n",
    "                           random_state=random_state)\n",
    "(trainX, valX, trainY, valY) = train_test_split(features, labels, test_size = 0.1)\n",
    "\n",
    "trainY = np_utils.to_categorical(trainY, 2)\n",
    "valY = np_utils.to_categorical(valY, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Training model...\n",
      "Train on 9455 samples, validate on 1051 samples\n",
      "Epoch 0\n",
      "9455/9455 [==============================] - 0s - loss: 0.2292 - val. loss: 0.1848\n",
      "Epoch 1\n",
      "9455/9455 [==============================] - 0s - loss: 0.1982 - val. loss: 0.1772\n",
      "Epoch 2\n",
      "9455/9455 [==============================] - 0s - loss: 0.1927 - val. loss: 0.1721\n",
      "Epoch 3\n",
      "9455/9455 [==============================] - 0s - loss: 0.1870 - val. loss: 0.1696\n",
      "Epoch 4\n",
      "9455/9455 [==============================] - 0s - loss: 0.1858 - val. loss: 0.1673\n",
      "1051/1051 [==============================] - 0s - loss: 0.1679 - acc.: 0.9486     \n",
      "score : 0.948863636364\n",
      "1051/1051 [==============================] - 0s     \n",
      "('ROC:', 0.82793751625246115)\n"
     ]
    }
   ],
   "source": [
    "input_dim = trainX.shape[1]\n",
    "output_dim = 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim, 32, init='lecun_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(32, 32, init='lecun_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(32, output_dim, init='lecun_uniform'))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "print(\"Building model...\")\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adadelta\")\n",
    "\n",
    "print(\"Training model...\")\n",
    "model.fit(trainX, trainY, nb_epoch=5, batch_size=16, validation_data=(valX, valY), verbose=1)\n",
    "\n",
    "score = model.evaluate(valX, valY, show_accuracy=True, verbose=1, batch_size=32)\n",
    "print 'score : %s' %score[1]\n",
    "\n",
    "valid_preds = model.predict_proba(valX, verbose=1)\n",
    "valid_preds = valid_preds[:, 0]\n",
    "roc = metrics.roc_auc_score(valY, valid_preds)\n",
    "print(\"ROC:\", roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116293/116293 [==============================] - 0s     \n"
     ]
    }
   ],
   "source": [
    "testX = preprocess_test(\"/Users/dikien/Downloads/West Nile Virus Prediction/test.csv\",\n",
    "                        \"/Users/dikien/Downloads/West Nile Virus Prediction/weather.csv\")\n",
    "\n",
    "valid_preds = model.predict_proba(testX, verbose=1)\n",
    "valid_preds = valid_preds[: , 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116293,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"/Users/dikien/Downloads/West Nile Virus Prediction/test.csv\", usecols=['Id'], dtype={'Id': np.int})\n",
    "df2 = pd.DataFrame({'WnvPresent' : valid_preds})\n",
    "df = df1.join(df2)\n",
    "df.to_csv('Submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
